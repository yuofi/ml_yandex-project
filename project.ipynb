{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from time import perf_counter\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "#from skimage import io, transform\n",
    "import os\n",
    "from torchvision.transforms import Compose, PILToTensor, Normalize\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "import torchshow as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),  # 256 x 256 x 16\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(2),  # 128 x 128 x 16\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),  # 128 x 128 x 32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(2),  # 64 x 64 x 32\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),  # 64 x 64 x 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(2),  # 32 x 32 x 64\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),  # 32 x 32 x 128\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(2),  # 16 x 16 x 128\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),  # 16 x 16 x 256\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(2),  # 8 x 8 x 256\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(in_features=8*8*256, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(in_features=1024, out_features=256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(in_features=256, out_features=64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(in_features=64, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n",
    "        self.act1  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n",
    "        self.act2  = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act2(self.conv2(self.act1(self.conv1(x))))\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(1,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = T.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(1,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, out_sz=(572,572)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "        self.out_sz  = out_sz\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights = torch.load('D:\\ml_yandex-project\\checkpoints\\model_10.pt')\n",
    "#optimizer_weights = torch.load('D:\\ml_yandex-project\\checkpoints\\optimizer_segment.pt')\n",
    "\n",
    "unet_model = UNet(enc_chs=(1,64,128,256,512), dec_chs=(512, 256, 128, 64), retain_dim=True, out_sz=(256,256)).to(device)\n",
    "classification_model = Model()\n",
    "\n",
    "unet_model.load_state_dict(model_weights)\n",
    "classification_model.load_state_dict(torch.load('D:\\ml_yandex-project\\checkpoints\\model_clsf.pt'))\n",
    "\n",
    "#optimizer_unet = torch.optim.Adam(unet_model.parameters())\n",
    "#optimizer_classification = torch.optim.Adam(classification_model.parameters())\n",
    "\n",
    "#optimizer_unet.load_state_dict(torch.load('D:\\ml_yandex-project\\checkpoints\\optimizer_segment.pt'))\n",
    "#optimizer_classification.load_state_dict(torch.load('D:\\ml_yandex-project\\checkpoints\\optimizer_clsf.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 %\n",
      "Маски успешно созданы и сохранены.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "image_folder = 'D:/ml_yandex-project/data/test_images/'\n",
    "\n",
    "mask_folder = 'D:/ml_yandex-project/data/test_lung_masks/'\n",
    "\n",
    "if not os.path.exists(mask_folder):\n",
    "    os.makedirs(mask_folder)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "count = 0\n",
    "for image_name in os.listdir(image_folder):\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    image = Image.open(image_path).convert('L')\n",
    "\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "    unet_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mask = unet_model(image_tensor)\n",
    "\n",
    "    mask_image = transforms.ToPILImage()(mask.squeeze(0).cpu())\n",
    "\n",
    "    mask_name = image_name.split('.')[0] + '.png'\n",
    "    mask_path = os.path.join(mask_folder, mask_name)\n",
    "    mask_image.save(mask_path)\n",
    "    if count % 100 == 0:\n",
    "        clear_output()\n",
    "        print(int(count / 6920 * 100), \"%\")\n",
    "    count += 1\n",
    "\n",
    "print(\"Маски успешно созданы и сохранены.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920 6920 6920\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "images = []\n",
    "names = []\n",
    "\n",
    "answers_dir = \"data/test_images\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classification_model.to(device)\n",
    "classification_model.eval()\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=9),\n",
    "    T.ColorJitter(brightness=0.2, \n",
    "                  contrast=0.2, \n",
    "                  saturation=0.2,\n",
    "                  hue=0.1),\n",
    "    T.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "    T.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    T.RandomGrayscale(p=0.1),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "names = []\n",
    "predictions = []\n",
    "\n",
    "answers_dir = \"data/test_images\"\n",
    "count = 0\n",
    "\n",
    "for filename in os.listdir(answers_dir):\n",
    "    img_path = os.path.join(answers_dir, filename)\n",
    "    img = Image.open(img_path)\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = classification_model(img)\n",
    "        predicted_class = torch.argmax(output, dim=1)\n",
    "        predictions.append(predicted_class.item())\n",
    "        names.append(count)\n",
    "        count += 1\n",
    "\n",
    "print(len(predictions), len(os.listdir(answers_dir)), len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\": names,\n",
    "    \"target_feature\": predictions\n",
    "})\n",
    "\n",
    "df.to_csv('answers.csv', index=False, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
